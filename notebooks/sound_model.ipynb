{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import pretty_midi\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_law_encode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device)\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device)\n",
    "    # audio = torch.abs(torch.tensor(input))\n",
    "    # audio_abs = torch.min(torch.abs(audio), 1.0)\n",
    "    mag = torch.log1p(mu * torch.abs(audio)) / torch.log1p(mu)\n",
    "    signal = torch.sign(audio) * mag\n",
    "    out = ((signal + 1) / 2 * mu + 0.5)\n",
    "    out = torch.tensor(out, device=device)\n",
    "    return out\n",
    "\n",
    "def mulaw_decode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device).item()\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device).item(),\n",
    "    audio = (audio / mu) * 2 - 1\n",
    "    out = torch.sign(audio) * (torch.exp(torch.abs(audio) * torch.log1p(mu)) - 1) / mu\n",
    "    return out\n",
    "\n",
    "# def one_hot_encode(input, channels=256):\n",
    "#     size = input.size()[0]\n",
    "#     one_hot = torch.ShortTensor(size, channels)\n",
    "#     one_hot = one_hot.zero_()\n",
    "#     in1 = torch.tensor(input, dtype=torch.long, device='cpu')\n",
    "#     one_hot.scatter_(1, in1.unsqueeze(1), 1.0)\n",
    "#     one_hot = one_hot.to(device)\n",
    "#     return one_hot\n",
    "\n",
    "\n",
    "# def one_hot_decode(input):\n",
    "#     _, i = input.max(1)\n",
    "#     return torch.tensor(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for transforming .midi audio to .wav "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from midi2audio import FluidSynth\n",
    "\n",
    "# fs = FluidSynth()\n",
    "# # fs.midi_to_audio('notebooks/test.midi', 'notebooks/test3.wav')\n",
    "# midi_dir = 'maestro-v1.0.0/2004'\n",
    "# wav_dir = 'dataset'\n",
    "\n",
    "# os.makedirs(wav_dir, exist_ok=True)\n",
    "\n",
    "# # Loop through all MIDI files in the directory\n",
    "# for midi_file in os.listdir(midi_dir):\n",
    "#     if midi_file.endswith('.midi'):\n",
    "#         midi_path = os.path.join(midi_dir, midi_file)\n",
    "#         wav_path = os.path.join(wav_dir, midi_file.replace('.midi', '.wav'))\n",
    "        \n",
    "#         # Convert MIDI to WAV\n",
    "#         fs.midi_to_audio(midi_path, wav_path)\n",
    "#         print(f\"Converted {midi_path} to {wav_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for creating 1sec chunks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "\n",
    "# # Parameters\n",
    "# sample_rate = 16000  # Desired sample rate\n",
    "# chunk_duration = 1  # Duration of each chunk in seconds\n",
    "# chunk_length = sample_rate * chunk_duration  # Number of samples per chunk\n",
    "\n",
    "# # Directories\n",
    "# input_dir = 'dataset'  # Folder with your 200 audio files\n",
    "# output_dir = 'data_chunks'  # Folder to save the chunks\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# file_idx = 0\n",
    "# # Loop through all audio files\n",
    "# for audio_file in os.listdir(input_dir):\n",
    "#     if audio_file.endswith('.wav'):\n",
    "#         # Load the audio file\n",
    "#         audio_path = os.path.join(input_dir, audio_file)\n",
    "#         audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "\n",
    "#         # Split into chunks\n",
    "#         num_chunks = len(audio) // chunk_length\n",
    "#         for i in range(num_chunks):\n",
    "#             start_sample = i * chunk_length\n",
    "#             end_sample = start_sample + chunk_length\n",
    "#             chunk = audio[start_sample:end_sample]\n",
    "\n",
    "#             # Save the chunk\n",
    "#             chunk_filename = f\"chunk{file_idx}_{i}.wav\"\n",
    "#             chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "#             sf.write(chunk_path, chunk, sample_rate)\n",
    "#             print(f\"Saved {chunk_path}\")\n",
    "#         file_idx += 1\n",
    "#         # Do not save last partial chunk\n",
    "# print(\"All audio files have been split into chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for creating X, y pairs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "\n",
    "# block_size = 16\n",
    "# sample_rate = 16000 \n",
    "# chunk_dir = 'data_chunks' \n",
    "# output_dir_X = 'processed_chunks_16/X' \n",
    "# output_dir_Y = 'processed_chunks_16/Y'\n",
    "# os.makedirs(output_dir_X, exist_ok=True)\n",
    "# os.makedirs(output_dir_Y, exist_ok=True)\n",
    "\n",
    "# def build_and_save_pairs(audio, block_size,base_filename):\n",
    "#     context = torch.zeros(block_size, dtype=torch.int16, device=device)\n",
    "#     idx = 0\n",
    "#     for i in range(len(audio) - block_size):\n",
    "#         target = audio[i + block_size]\n",
    "#         X = torch.tensor(context, dtype=torch.int16, device=device)\n",
    "#         Y = torch.tensor(target, dtype=torch.int16, device=device)\n",
    "#         context = torch.cat((context[1:], torch.tensor([target], dtype=torch.int16, device=device)))\n",
    "\n",
    "#         X_filename = f\"{base_filename}_X_{idx}.pt\"\n",
    "#         Y_filename = f\"{base_filename}_Y_{idx}.pt\"\n",
    "#         X_path = os.path.join(output_dir_X, X_filename)\n",
    "#         Y_path = os.path.join(output_dir_Y, Y_filename)\n",
    "\n",
    "#         torch.save(X, X_path)\n",
    "#         torch.save(Y, Y_path)\n",
    "#         print(f\"Saved {X_path} and {Y_path}\")\n",
    "#         idx += 1\n",
    "\n",
    "\n",
    "# for chunk_file in os.listdir(chunk_dir):\n",
    "#     if chunk_file.endswith('.wav'):\n",
    "#         chunk_path = os.path.join(chunk_dir, chunk_file)\n",
    "        \n",
    "#         audio, sr = librosa.load(chunk_path, sr=sample_rate)\n",
    "\n",
    "#         audio = mu_law_encode(audio)\n",
    "\n",
    "#         base_filename = os.path.splitext(chunk_file)[0]\n",
    "        \n",
    "#         build_and_save_pairs(audio, block_size, base_filename)\n",
    "\n",
    "# print(\"Finished processing all chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check some X,y pairs\n",
    "# for i in range(100):\n",
    "#     X = torch.load(f'processed_chunks/X/chunk87_272_X_{i}.pt')\n",
    "#     y = torch.load(f'processed_chunks/Y/chunk87_272_Y_{i}.pt')\n",
    "#     print(f\"{X.tolist()} -> {y.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), device=device) / (fan_in**0.5) \n",
    "        self.bias = torch.zeros(fan_out, device=device) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "\n",
    "        self.running_mean = torch.zeros(dim, device=device)\n",
    "        self.running_var = torch.ones(dim, device=device)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "\n",
    "            xmean = x.mean(dim=dim, keepdim=True)\n",
    "            xvar = x.var(dim=dim, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class LeakyRelu:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.nn.functional.leaky_relu(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Relu:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.nn.functional.relu(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params, lr=0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "        self.beta1 = torch.tensor(beta1, device=device)\n",
    "        self.beta2 = torch.tensor(beta2, device=device)\n",
    "        self.eps = eps\n",
    "        self.m_d = {id(p) : torch.zeros_like(p) for p in params}\n",
    "        self.v_d = {id(p) : torch.zeros_like(p) for p in params}\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            grad = param.grad\n",
    "            m = self.m_d[id(param)]\n",
    "            v = self.v_d[id(param)]\n",
    "\n",
    "            next_m = (torch.multiply(self.beta1, m) + torch.multiply(1.0 - self.beta1, grad))\n",
    "            next_v = (torch.multiply(self.beta2, v) + torch.multiply(1.0 - self.beta2, torch.pow(grad, 2)))\n",
    "\n",
    "            m_hat = torch.divide(next_m, (1 - torch.pow(self.beta1, self.t)))\n",
    "            v_hat = torch.divide(next_v, (1 - torch.pow(self.beta2, self.t)))\n",
    "\n",
    "            param.data = param.data - torch.divide(torch.multiply(self.lr, m_hat), (torch.sqrt(v_hat) + self.eps))\n",
    "\n",
    "            self.m_d[id(param)] = next_m\n",
    "            self.v_d[id(param)] = next_v\n",
    "        self.t += 1\n",
    "    \n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "\n",
    "# n_embd = 24\n",
    "# n_hidden = 128\n",
    "# vocab_size = 256\n",
    "\n",
    "# model = Sequential([\n",
    "#     Flatten(2), Linear(vocab_size * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Linear(n_hidden, vocab_size),\n",
    "# ])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.layers[-1].weight *= 0.1\n",
    "\n",
    "# # parameters = model.parameters()\n",
    "# # print(sum(p.nelement() for p in parameters))\n",
    "# # for p in parameters:\n",
    "# #     p.requires_grad = True\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import soundfile as sf \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X_dir, Y_dir):\n",
    "        self.X_dir = X_dir\n",
    "        self.Y_dir = Y_dir\n",
    "        self.X_files = sorted(os.listdir(X_dir))\n",
    "        self.Y_files = sorted(os.listdir(Y_dir))\n",
    "        \n",
    "        # Check if X and Y files match in length\n",
    "        assert len(self.X_files) == len(self.Y_files), \"Mismatch between X and Y files\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Total number of samples in the dataset\n",
    "        return len(self.X_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the X and Y tensors for the given index\n",
    "        X_path = os.path.join(self.X_dir, self.X_files[idx])\n",
    "        Y_path = os.path.join(self.Y_dir, self.Y_files[idx])\n",
    "        \n",
    "        # Load tensors from files\n",
    "        X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
    "        Y = torch.load(Y_path)  # Convert to long if needed\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.long)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "        X_one_hot = torch.nn.functional.one_hot(X, 256).float()\n",
    "        Y_one_hot = torch.nn.functional.one_hot(Y, 256).float()\n",
    "        \n",
    "        return X_one_hot, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/work/WaveNet_implementation/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98446/2758831217.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
      "/tmp/ipykernel_98446/2758831217.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(Y_path)  # Convert to long if needed\n",
      "/tmp/ipykernel_98446/2758831217.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.long)\n",
      "/tmp/ipykernel_98446/2758831217.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n",
      "torch.Size([64])\n",
      "Input size: torch.Size([64, 16, 256])\n",
      "Output size after transpose: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "Output size after conv: torch.Size([64, 512, 15])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual conv: torch.Size([64, 512, 15])\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 512, 15])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 512, 15])\n",
      "Output size after conv: torch.Size([64, 512, 14])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 512, 14])\n",
      "Output size after tanh: torch.Size([64, 512, 14])\n",
      "Output size after sigmoid: torch.Size([64, 512, 14])\n",
      "Output size after gated: torch.Size([64, 512, 14])\n",
      "Output size after res conv1d: torch.Size([64, 512, 14])\n",
      "Input cut size: torch.Size([64, 512, 14])\n",
      "Residual output size: torch.Size([64, 512, 14])\n",
      "Skip size: torch.Size([64, 256, 14])\n",
      "Residual output size: torch.Size([64, 512, 14])\n",
      "Skip size: torch.Size([64, 256, 12])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 512, 14])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 512, 14])\n",
      "Output size after conv: torch.Size([64, 512, 13])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 512, 13])\n",
      "Output size after tanh: torch.Size([64, 512, 13])\n",
      "Output size after sigmoid: torch.Size([64, 512, 13])\n",
      "Output size after gated: torch.Size([64, 512, 13])\n",
      "Output size after res conv1d: torch.Size([64, 512, 13])\n",
      "Input cut size: torch.Size([64, 512, 13])\n",
      "Residual output size: torch.Size([64, 512, 13])\n",
      "Skip size: torch.Size([64, 256, 13])\n",
      "Residual output size: torch.Size([64, 512, 13])\n",
      "Skip size: torch.Size([64, 256, 12])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 512, 13])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 512, 13])\n",
      "Output size after conv: torch.Size([64, 512, 12])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 512, 12])\n",
      "Output size after tanh: torch.Size([64, 512, 12])\n",
      "Output size after sigmoid: torch.Size([64, 512, 12])\n",
      "Output size after gated: torch.Size([64, 512, 12])\n",
      "Output size after res conv1d: torch.Size([64, 512, 12])\n",
      "Input cut size: torch.Size([64, 512, 12])\n",
      "Residual output size: torch.Size([64, 512, 12])\n",
      "Skip size: torch.Size([64, 256, 12])\n",
      "Residual output size: torch.Size([64, 512, 12])\n",
      "Skip size: torch.Size([64, 256, 12])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 512, 12])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 512, 12])\n",
      "Output size after conv: torch.Size([64, 512, 11])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 512, 11])\n",
      "Output size after tanh: torch.Size([64, 512, 11])\n",
      "Output size after sigmoid: torch.Size([64, 512, 11])\n",
      "Output size after gated: torch.Size([64, 512, 11])\n",
      "Output size after res conv1d: torch.Size([64, 512, 11])\n",
      "Input cut size: torch.Size([64, 512, 11])\n",
      "Residual output size: torch.Size([64, 512, 11])\n",
      "Skip size: torch.Size([64, 256, 11])\n",
      "Residual output size: torch.Size([64, 512, 11])\n",
      "Skip size: torch.Size([64, 256, 11])\n",
      "----------RESIDUAL BLOCK--------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [64, 256, 12] at entry 0 and [64, 256, 11] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Y_batch)\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:279\u001b[0m, in \u001b[0;36mWaveNet.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:270\u001b[0m, in \u001b[0;36mWaveNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    268\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcasualConv1d(output)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput size after casual conv: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 270\u001b[0m skip_connections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstackResidualBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip connections size after stack residual block: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskip_connections\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    272\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(skip_connections, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:179\u001b[0m, in \u001b[0;36mStackResidualBlock.__call__\u001b[0;34m(self, x, skipSize)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, skipSize):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipSize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:176\u001b[0m, in \u001b[0;36mStackResidualBlock.forward\u001b[0;34m(self, inputX, skipSize)\u001b[0m\n\u001b[1;32m    174\u001b[0m     residual_output, skip_output \u001b[38;5;241m=\u001b[39m res_block(residual_output, skipSize)\n\u001b[1;32m    175\u001b[0m     skip_outputs\u001b[38;5;241m.\u001b[39mappend(skip_output)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m residual_output, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [64, 256, 12] at entry 0 and [64, 256, 11] at entry 3"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from layers import WaveNet\n",
    "\n",
    "X_dir = '../processed_chunks_16/X'\n",
    "Y_dir = '../processed_chunks_16/Y'\n",
    "ud, lossi = [], []\n",
    "\n",
    "model = WaveNet(1, 4, 256, 512)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_steps = 2000\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "audio_dataset = AudioDataset(X_dir, Y_dir)\n",
    "train_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    if step >= num_steps:\n",
    "        break\n",
    "    # Move batches to the desired device, e.g., GPU if available\n",
    "    # X_batch = torch.unsqueeze(X_batch, dim=1)\n",
    "    X_batch = X_batch.to('cpu')\n",
    "    # Y_batch = Y_batch.type(torch.LongTensor)\n",
    "    Y_batch = Y_batch.to('cpu')\n",
    "    print(X_batch.shape)\n",
    "    print(Y_batch.shape)\n",
    "    logits = model(X_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.cross_entropy(logits, Y_batch)\n",
    "    # for p in model.parameters():\n",
    "    #     p.grad = None\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"{step:7d}/{num_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in model.parameters()])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch.shape, Y_batch.shape, model.layers[1].weight.shape, logits.shape, logits\n",
    "X_batch.shape, Y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.dtype, Y_batch.dtype, model.layers[1].weight.dtype, logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_batch, Y_batch):\n",
    "    print(f\"X: {x.tolist()} -> Y: {y.item()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt      \n",
    "import numpy as np\n",
    "import torch    \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import librosa\n",
    "\n",
    "import librosa.display\n",
    "#from datasets import WaveNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1683).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_generate = 16000\n",
    "context = [0] * 8\n",
    "\n",
    "generated_samples = []\n",
    "\n",
    "for i in range(num_samples_to_generate):\n",
    "    # Forward pass: Get predictions for the current context\n",
    "    context_tensor = torch.nn.functional.one_hot(torch.tensor(context), 256).float().to(device)\n",
    "    logits = model(context_tensor.unsqueeze(0))  # Forward pass\n",
    "    probs = F.softmax(logits, dim=1)                          # Softmax for probabilities\n",
    "\n",
    "    # Sample from the probability distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()        # Sample next audio sample\n",
    "    generated_samples.append(ix)                               # Store generated sample\n",
    "\n",
    "    # Update the context window by shifting and adding the new sample\n",
    "    context = context[1:] + [ix] \n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i:7d}/{num_samples_to_generate:7d}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(generated_samples), min(generated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulaw_decode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device)\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device)\n",
    "    audio = (audio / mu) * 2 - 1\n",
    "    out = torch.sign(audio) * (torch.exp(torch.abs(audio) * torch.log1p(mu)) - 1) / mu\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_audio = mulaw_decode(torch.tensor(generated_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print max from decoded audio\n",
    "print(decoded_audio.max())\n",
    "print(decoded_audio.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io.wavfile import write\n",
    "\n",
    "# write('generated_audio.wav', 16000, decoded_audio.numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Play audio in Jupyter Notebook\n",
    "Audio(decoded_audio.cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Convert decoded_audio to a NumPy array if itâ€™s a tensor\n",
    "if isinstance(decoded_audio, torch.Tensor):\n",
    "    decoded_audio_np = decoded_audio.cpu().numpy()  # Ensure it's on CPU and convert to numpy\n",
    "else:\n",
    "    decoded_audio_np = decoded_audio  # If it's already a numpy array, keep it as is\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(decoded_audio_np, sr=16000)  # Assuming a sample rate of 16kHz\n",
    "plt.title('Generated Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylim(-0.03, 0.03)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example audio from data_chunks .wav\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = 'data_chunks/chunk1_100.wav'\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(audio, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavenet-implementation-74K-dzoQ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
