{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import pretty_midi\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_law_encode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device)\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device)\n",
    "    # audio = torch.abs(torch.tensor(input))\n",
    "    # audio_abs = torch.min(torch.abs(audio), 1.0)\n",
    "    mag = torch.log1p(mu * torch.abs(audio)) / torch.log1p(mu)\n",
    "    signal = torch.sign(audio) * mag\n",
    "    out = ((signal + 1) / 2 * mu + 0.5)\n",
    "    out = torch.tensor(out, device=device)\n",
    "    return out\n",
    "\n",
    "def mulaw_decode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device).item()\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device).item(),\n",
    "    audio = (audio / mu) * 2 - 1\n",
    "    out = torch.sign(audio) * (torch.exp(torch.abs(audio) * torch.log1p(mu)) - 1) / mu\n",
    "    return out\n",
    "\n",
    "# def one_hot_encode(input, channels=256):\n",
    "#     size = input.size()[0]\n",
    "#     one_hot = torch.ShortTensor(size, channels)\n",
    "#     one_hot = one_hot.zero_()\n",
    "#     in1 = torch.tensor(input, dtype=torch.long, device='cpu')\n",
    "#     one_hot.scatter_(1, in1.unsqueeze(1), 1.0)\n",
    "#     one_hot = one_hot.to(device)\n",
    "#     return one_hot\n",
    "\n",
    "\n",
    "# def one_hot_decode(input):\n",
    "#     _, i = input.max(1)\n",
    "#     return torch.tensor(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for transforming .midi audio to .wav "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from midi2audio import FluidSynth\n",
    "\n",
    "# fs = FluidSynth()\n",
    "# # fs.midi_to_audio('notebooks/test.midi', 'notebooks/test3.wav')\n",
    "# midi_dir = 'maestro-v1.0.0/2004'\n",
    "# wav_dir = 'dataset'\n",
    "\n",
    "# os.makedirs(wav_dir, exist_ok=True)\n",
    "\n",
    "# # Loop through all MIDI files in the directory\n",
    "# for midi_file in os.listdir(midi_dir):\n",
    "#     if midi_file.endswith('.midi'):\n",
    "#         midi_path = os.path.join(midi_dir, midi_file)\n",
    "#         wav_path = os.path.join(wav_dir, midi_file.replace('.midi', '.wav'))\n",
    "        \n",
    "#         # Convert MIDI to WAV\n",
    "#         fs.midi_to_audio(midi_path, wav_path)\n",
    "#         print(f\"Converted {midi_path} to {wav_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for creating 1sec chunks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "\n",
    "# # Parameters\n",
    "# sample_rate = 16000  # Desired sample rate\n",
    "# chunk_duration = 1  # Duration of each chunk in seconds\n",
    "# chunk_length = sample_rate * chunk_duration  # Number of samples per chunk\n",
    "\n",
    "# # Directories\n",
    "# input_dir = 'dataset'  # Folder with your 200 audio files\n",
    "# output_dir = 'data_chunks'  # Folder to save the chunks\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# file_idx = 0\n",
    "# # Loop through all audio files\n",
    "# for audio_file in os.listdir(input_dir):\n",
    "#     if audio_file.endswith('.wav'):\n",
    "#         # Load the audio file\n",
    "#         audio_path = os.path.join(input_dir, audio_file)\n",
    "#         audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "\n",
    "#         # Split into chunks\n",
    "#         num_chunks = len(audio) // chunk_length\n",
    "#         for i in range(num_chunks):\n",
    "#             start_sample = i * chunk_length\n",
    "#             end_sample = start_sample + chunk_length\n",
    "#             chunk = audio[start_sample:end_sample]\n",
    "\n",
    "#             # Save the chunk\n",
    "#             chunk_filename = f\"chunk{file_idx}_{i}.wav\"\n",
    "#             chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "#             sf.write(chunk_path, chunk, sample_rate)\n",
    "#             print(f\"Saved {chunk_path}\")\n",
    "#         file_idx += 1\n",
    "#         # Do not save last partial chunk\n",
    "# print(\"All audio files have been split into chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for creating X, y pairs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "\n",
    "# block_size = 16\n",
    "# sample_rate = 16000 \n",
    "# chunk_dir = 'data_chunks' \n",
    "# output_dir_X = 'processed_chunks_16/X' \n",
    "# output_dir_Y = 'processed_chunks_16/Y'\n",
    "# os.makedirs(output_dir_X, exist_ok=True)\n",
    "# os.makedirs(output_dir_Y, exist_ok=True)\n",
    "\n",
    "# def build_and_save_pairs(audio, block_size,base_filename):\n",
    "#     context = torch.zeros(block_size, dtype=torch.int16, device=device)\n",
    "#     idx = 0\n",
    "#     for i in range(len(audio) - block_size):\n",
    "#         target = audio[i + block_size]\n",
    "#         X = torch.tensor(context, dtype=torch.int16, device=device)\n",
    "#         Y = torch.tensor(target, dtype=torch.int16, device=device)\n",
    "#         context = torch.cat((context[1:], torch.tensor([target], dtype=torch.int16, device=device)))\n",
    "\n",
    "#         X_filename = f\"{base_filename}_X_{idx}.pt\"\n",
    "#         Y_filename = f\"{base_filename}_Y_{idx}.pt\"\n",
    "#         X_path = os.path.join(output_dir_X, X_filename)\n",
    "#         Y_path = os.path.join(output_dir_Y, Y_filename)\n",
    "\n",
    "#         torch.save(X, X_path)\n",
    "#         torch.save(Y, Y_path)\n",
    "#         print(f\"Saved {X_path} and {Y_path}\")\n",
    "#         idx += 1\n",
    "\n",
    "\n",
    "# for chunk_file in os.listdir(chunk_dir):\n",
    "#     if chunk_file.endswith('.wav'):\n",
    "#         chunk_path = os.path.join(chunk_dir, chunk_file)\n",
    "        \n",
    "#         audio, sr = librosa.load(chunk_path, sr=sample_rate)\n",
    "\n",
    "#         audio = mu_law_encode(audio)\n",
    "\n",
    "#         base_filename = os.path.splitext(chunk_file)[0]\n",
    "        \n",
    "#         build_and_save_pairs(audio, block_size, base_filename)\n",
    "\n",
    "# print(\"Finished processing all chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check some X,y pairs\n",
    "# for i in range(100):\n",
    "#     X = torch.load(f'processed_chunks/X/chunk87_272_X_{i}.pt')\n",
    "#     y = torch.load(f'processed_chunks/Y/chunk87_272_Y_{i}.pt')\n",
    "#     print(f\"{X.tolist()} -> {y.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), device=device) / (fan_in**0.5) \n",
    "        self.bias = torch.zeros(fan_out, device=device) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "\n",
    "        self.running_mean = torch.zeros(dim, device=device)\n",
    "        self.running_var = torch.ones(dim, device=device)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "\n",
    "            xmean = x.mean(dim=dim, keepdim=True)\n",
    "            xvar = x.var(dim=dim, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class LeakyRelu:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.nn.functional.leaky_relu(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Relu:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.nn.functional.relu(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params, lr=0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "        self.beta1 = torch.tensor(beta1, device=device)\n",
    "        self.beta2 = torch.tensor(beta2, device=device)\n",
    "        self.eps = eps\n",
    "        self.m_d = {id(p) : torch.zeros_like(p) for p in params}\n",
    "        self.v_d = {id(p) : torch.zeros_like(p) for p in params}\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            grad = param.grad\n",
    "            m = self.m_d[id(param)]\n",
    "            v = self.v_d[id(param)]\n",
    "\n",
    "            next_m = (torch.multiply(self.beta1, m) + torch.multiply(1.0 - self.beta1, grad))\n",
    "            next_v = (torch.multiply(self.beta2, v) + torch.multiply(1.0 - self.beta2, torch.pow(grad, 2)))\n",
    "\n",
    "            m_hat = torch.divide(next_m, (1 - torch.pow(self.beta1, self.t)))\n",
    "            v_hat = torch.divide(next_v, (1 - torch.pow(self.beta2, self.t)))\n",
    "\n",
    "            param.data = param.data - torch.divide(torch.multiply(self.lr, m_hat), (torch.sqrt(v_hat) + self.eps))\n",
    "\n",
    "            self.m_d[id(param)] = next_m\n",
    "            self.v_d[id(param)] = next_v\n",
    "        self.t += 1\n",
    "    \n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "\n",
    "# n_embd = 24\n",
    "# n_hidden = 128\n",
    "# vocab_size = 256\n",
    "\n",
    "# model = Sequential([\n",
    "#     Flatten(2), Linear(vocab_size * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Linear(n_hidden, vocab_size),\n",
    "# ])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.layers[-1].weight *= 0.1\n",
    "\n",
    "# # parameters = model.parameters()\n",
    "# # print(sum(p.nelement() for p in parameters))\n",
    "# # for p in parameters:\n",
    "# #     p.requires_grad = True\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import soundfile as sf \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X_dir, Y_dir):\n",
    "        self.X_dir = X_dir\n",
    "        self.Y_dir = Y_dir\n",
    "        self.X_files = sorted(os.listdir(X_dir))\n",
    "        self.Y_files = sorted(os.listdir(Y_dir))\n",
    "        \n",
    "        # Check if X and Y files match in length\n",
    "        assert len(self.X_files) == len(self.Y_files), \"Mismatch between X and Y files\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Total number of samples in the dataset\n",
    "        return len(self.X_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the X and Y tensors for the given index\n",
    "        X_path = os.path.join(self.X_dir, self.X_files[idx])\n",
    "        Y_path = os.path.join(self.Y_dir, self.Y_files[idx])\n",
    "        \n",
    "        # Load tensors from files\n",
    "        X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
    "        Y = torch.load(Y_path)  # Convert to long if needed\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.long)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "        X_one_hot = torch.nn.functional.one_hot(X, 256).float()\n",
    "        Y_one_hot = torch.nn.functional.one_hot(Y, 256).float()\n",
    "        \n",
    "        return X_one_hot,  Y_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/work/WaveNet_implementation/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26927/1059457871.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
      "/tmp/ipykernel_26927/1059457871.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(Y_path)  # Convert to long if needed\n",
      "/tmp/ipykernel_26927/1059457871.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.long)\n",
      "/tmp/ipykernel_26927/1059457871.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y, dtype=torch.long)\n",
      "/home/mikolaj/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8192x1 and 242x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 60\u001b[0m\n\u001b[1;32m     54\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust input shape to (batch_size, 256, seq_len)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(\"X_batch shape:\", X_batch.shape)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print(\"Y_batch shape:\", Y_batch.shape)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m FIRST\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:147\u001b[0m, in \u001b[0;36mWaveNetClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# x: (batch_size, 256, seq_length)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwavenet(x)\n\u001b[0;32m--> 147\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# Apply softmax if needed for a classification problem\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8192x1 and 242x1)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from layers import WaveNet, WaveNetClassifier  # Make sure you have imported the correct WaveNet class\n",
    "\n",
    "# Ensure you are appending the correct path for the 'layers' module\n",
    "\n",
    "\n",
    "# Directories\n",
    "X_dir = '../processed_chunks_16/X'\n",
    "Y_dir = '../processed_chunks_16/Y'\n",
    "\n",
    "# Lists to keep track of loss and gradient statistics\n",
    "ud, lossi = [], []\n",
    "\n",
    "# Instantiate the model (you can adjust the parameters based on your task)\n",
    "# model = WaveNet(256, 256, 2, 2, 3)  # Adjust this based on your configuration\n",
    "model = WaveNetClassifier(256, 1)\n",
    "model.to('cuda')  # Make sure to put model on the correct device, use 'cuda' if you want to use GPU\n",
    "model.train()  # Ensure model is in training mode\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_steps = 2000\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "audio_dataset = AudioDataset(X_dir, Y_dir)  # Ensure AudioDataset is implemented correctly\n",
    "train_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "for step, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    if step >= num_steps:\n",
    "        break\n",
    "\n",
    "    # Move batches to the desired device, e.g., GPU if available\n",
    "    X_batch = X_batch.to('cuda')  # Move to GPU if using CUDA (e.g., .to('cuda'))\n",
    "    Y_batch = Y_batch.to('cuda')  # Same for Y_batch\n",
    "\n",
    "    # print(\"X_batch shape:\", X_batch.shape)\n",
    "    # print(\"Y_batch shape:\", Y_batch.shape)\n",
    "\n",
    "    # Permute to match input shape expected by the model (batch_size, channels, sequence_length)\n",
    "    X_batch = X_batch.permute(0, 2, 1)  # Adjust input shape to (batch_size, 256, seq_len)\n",
    "\n",
    "    # print(\"X_batch shape:\", X_batch.shape)\n",
    "    # print(\"Y_batch shape:\", Y_batch.shape)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    logits = model(X_batch)\n",
    "    print(f\"{logits} FIRST\")\n",
    "    logits = logits.squeeze(-1)\n",
    "    print(f\"{logits} SECOND\")\n",
    "    Y_batch = Y_batch.argmax(dim=-1) \n",
    "\n",
    "    # print(\"logits shape:\", logits.shape)  # Should be (batch_size, num_classes, sequence_length)\n",
    "    # print(\"Y_batch shape:\", Y_batch.shape)  # Should be (batch_size, sequence_length)\n",
    "\n",
    "    # Reset gradients before backward pass\n",
    "\n",
    "\n",
    "    loss = loss_fn(logits, Y_batch)  # Remove extra dimensions if necessary\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    # Loss calculation: Use MSELoss for regression tasks\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters\n",
    "    # optimizer.step()\n",
    "    lr = 0.1\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    # Print the loss every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        print(f\"{step:7d}/{num_steps:7d}: {loss.item():.4f}\")\n",
    "\n",
    "    # Track logs\n",
    "    lossi.append(loss.log10().item())  # Store the log10 of loss\n",
    "    # with torch.no_grad():\n",
    "    #     ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for casualConv1D.conv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_0.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_0.resConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_0.resConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_0.skipConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_0.skipConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_1.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_1.resConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_1.resConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_1.skipConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_1.skipConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_2.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_2.resConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_2.resConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_2.skipConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_0_2.skipConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_0.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_0.resConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_0.resConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_0.skipConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_0.skipConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_1.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_1.resConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_1.resConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_1.skipConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_1.skipConv1D.bias: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_2.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_2.resConv1D.weight are None\n",
      "Gradients for stackResBlock.resBlock_1_2.resConv1D.bias are None\n",
      "Gradients for stackResBlock.resBlock_1_2.skipConv1D.weight: 0.0\n",
      "Gradients for stackResBlock.resBlock_1_2.skipConv1D.bias: 0.0\n",
      "Gradients for denseLayer.conv1d.weight: 0.0\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(f\"Gradients for {name} are None\")\n",
    "    else:\n",
    "        print(f\"Gradients for {name}: {param.grad.norm().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch shape: torch.Size([64, 256, 16])\n",
      "Y_batch shape: torch.Size([64, 256])\n",
      "Sample Y_batch: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(f\"logits shape: {logits.shape}\")  # Should be (batch_size, 256)\n",
    "print(f\"X_batch shape: {X_batch.shape}\")  # Should be (batch_size, 256)\n",
    "print(f\"Y_batch shape: {Y_batch.shape}\")  # Should be (batch_size,)\n",
    "print(f\"Sample Y_batch: {Y_batch[:5]}\")  # Print first few values\n",
    "# print(f\"Sample logits: {logits[:5].argmax(dim=-1)}\")  # Print predicted classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(model.wavenet.calculateReceptiveField())  # Check the receptive field size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "df=pd.read_csv(\"../synthetic_signal_data.csv\")\n",
    "df['signal']=df['signal']/df['signal'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['open_channels'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesereisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df:pd.Series(), seqLen=10):\n",
    "        super().__init__()\n",
    "        self.df=df\n",
    "        self.seqLen=seqLen\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]-self.seqLen-1\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        x=self.df.iloc[index:index+self.seqLen,1].values\n",
    "        y=self.df.iloc[index+self.seqLen-1,2] \n",
    "        return x,y   \n",
    "seqLen=50\n",
    "timeSeriesDataset = TimesereisDataset(df,seqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainNumbers=int(len(timeSeriesDataset)*0.9)\n",
    "trainDataset,testDataset=torch.utils.data.random_split(timeSeriesDataset,[trainNumbers,len(timeSeriesDataset)-trainNumbers])\n",
    "trainDataLoader=torch.utils.data.DataLoader(trainDataset,batch_size=8,shuffle=True)\n",
    "testDataLoader=torch.utils.data.DataLoader(testDataset,batch_size=8,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n",
      "Validation: 201it [00:01, 159.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 0 : 2.70417985156994  :  Accuracy: 0.9950248756218906 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:01, 155.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 500 : 0.5785397070260784  :  Accuracy: 90.04975124378109 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:01, 152.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 1000 : 0.5714088171275694  :  Accuracy: 90.29850746268657 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m x_train\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m y_train\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 28\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mwavenetClassifierModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(output,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m=\u001b[39m lossFunction(output,y_train)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/work/WaveNet_from_scratch/layers.py:147\u001b[0m, in \u001b[0;36mWaveNetClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# print(\"Input shape:\", x.shape)return np.sum([(self.kernel_size - 1) * (2 ** l) for l in range(self.layer_size)] * self.stack_size)\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# print(\"After WaveNet shape:\", x.shape)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# x = x.view(x.size(0), -1)  # Flatten before Linear\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# print(\"Before Linear shape:\", x.shape)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mliner(x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/work/WaveNet_from_scratch/layers.py:131\u001b[0m, in \u001b[0;36mWaveNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcasualConv1D(x)\n\u001b[1;32m    130\u001b[0m skipSize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculateOutputSize(x)\n\u001b[0;32m--> 131\u001b[0m _, skipConnections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstackResBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipSize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m dense \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenseLayer(skipConnections)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dense\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/work/WaveNet_from_scratch/layers.py:103\u001b[0m, in \u001b[0;36mStackOfResBlocks.forward\u001b[0;34m(self, x, skipSize)\u001b[0m\n\u001b[1;32m    101\u001b[0m skipOutputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resBlock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresBlocks:\n\u001b[0;32m--> 103\u001b[0m     resOutput, skipOutput \u001b[38;5;241m=\u001b[39m \u001b[43mresBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresOutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipSize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     skipOutputs\u001b[38;5;241m.\u001b[39mappend(skipOutput)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#print(\"resOutput size:\", resOutput.size())\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#print(\"skipOutputs size:\", skipOutputs[0].size())\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/work/WaveNet_from_scratch/layers.py:66\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, inputX, skipSize)\u001b[0m\n\u001b[1;32m     64\u001b[0m resOutput \u001b[38;5;241m=\u001b[39m resOutput \u001b[38;5;241m+\u001b[39m inputX[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39mresOutput\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m):]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#print(\"resOutput size:\", resOutput.size())\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m skipOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskipConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#print(\"skipOutput size:\", skipOutput.size())\u001b[39;00m\n\u001b[1;32m     68\u001b[0m skipOutput \u001b[38;5;241m=\u001b[39m skipOutput[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39mskipSize:]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-tchtkN5J-py3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from layers import WaveNet,WaveNetClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "wavenetClassifierModel=WaveNetClassifier(seqLen,df['open_channels'].max()+1)\n",
    "wavenetClassifierModel.to(device)\n",
    "\n",
    "wavenetClassifierModel.train()\n",
    "\n",
    "optimizer=torch.optim.AdamW(wavenetClassifierModel.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "lossFunction = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def calc_accuracy(Out,Y):\n",
    "    max_vals, max_indices = torch.max(Out,1)\n",
    "    train_acc = (max_indices == Y).sum().item()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "epochs=1\n",
    "globalStep=500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_train,y_train) in enumerate(trainDataLoader):\n",
    "         x_train = torch.unsqueeze(x_train,dim=1).float()\n",
    "         x_train.to(device)\n",
    "         y_train.to(device)\n",
    "         output=wavenetClassifierModel(x_train)\n",
    "         output = torch.squeeze(output,dim=1)\n",
    "\n",
    "         loss= lossFunction(output,y_train)\n",
    "         optimizer.zero_grad()\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         if step%globalStep==0:\n",
    "            # scheduler.step()\n",
    "            # print(output.detach().numpy())\n",
    "            # print(y_train.numpy())\n",
    "            with torch.no_grad():\n",
    "                accuracy=0\n",
    "                loss=0\n",
    "                for stepTest, (x_test,y_test) in tqdm(enumerate(testDataLoader),desc=\"Validation\"):\n",
    "                    x_test.to(device)\n",
    "                    y_test.to(device)\n",
    "                    x_test = torch.unsqueeze(x_test,dim=1).float()\n",
    "                    output=wavenetClassifierModel(x_test)\n",
    "                    output = torch.squeeze(output,dim=1)\n",
    "                    accuracy+=calc_accuracy(output,y_test)*100\n",
    "                    loss+= lossFunction(output,y_test).item()\n",
    "                    if stepTest>200:\n",
    "                        break\n",
    "            print(f\"loss for step {step} : {loss/stepTest}  :  Accuracy: {accuracy/stepTest} %\")\n",
    "\n",
    "         \n",
    "    print(f\"epch {epoch}\")\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.sum([(2 - 1) * (2 ** l) for l in range(4)] * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casualConv1D.conv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_0.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_0.resConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_0.resConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_0_0.skipConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_0.skipConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_0_1.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_1.resConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_1.resConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_0_1.skipConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_1.skipConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_0_2.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_2.resConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_2.resConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_0_2.skipConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_0_2.skipConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_1_0.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_0.resConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_0.resConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_1_0.skipConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_0.skipConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_1_1.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_1.resConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_1.resConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_1_1.skipConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_1.skipConv1D.bias: 0.0\n",
      "stackResBlock.resBlock_1_2.casualDilatedConv1D.conv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_2.skipConv1D.weight: 0.0\n",
      "stackResBlock.resBlock_1_2.skipConv1D.bias: 0.0\n",
      "denseLayer.conv1d.weight: 0.0\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        print(f\"{name}: {param.grad.norm().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65224/2758831217.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
      "/tmp/ipykernel_65224/2758831217.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(Y_path)  # Convert to long if needed\n",
      "/tmp/ipykernel_65224/2758831217.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.long)\n",
      "/tmp/ipykernel_65224/2758831217.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 16])\n",
      "torch.Size([64])\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "Output size after conv: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual conv: torch.Size([64, 256, 16])\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "Output size after conv: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 16])\n",
      "Output size after tanh: torch.Size([64, 256, 16])\n",
      "Output size after sigmoid: torch.Size([64, 256, 16])\n",
      "Output size after gated: torch.Size([64, 256, 16])\n",
      "Output size after res conv1d: torch.Size([64, 256, 16])\n",
      "Input cut size: torch.Size([64, 256, 16])\n",
      "Residual output size: torch.Size([64, 256, 16])\n",
      "Skip size before slicing: torch.Size([64, 256, 16])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "Output size after conv: torch.Size([64, 256, 15])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 15])\n",
      "Output size after tanh: torch.Size([64, 256, 15])\n",
      "Output size after sigmoid: torch.Size([64, 256, 15])\n",
      "Output size after gated: torch.Size([64, 256, 15])\n",
      "Output size after res conv1d: torch.Size([64, 256, 15])\n",
      "Input cut size: torch.Size([64, 256, 15])\n",
      "Residual output size: torch.Size([64, 256, 15])\n",
      "Skip size before slicing: torch.Size([64, 256, 15])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 15])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 15])\n",
      "Output size after conv: torch.Size([64, 256, 12])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 12])\n",
      "Output size after tanh: torch.Size([64, 256, 12])\n",
      "Output size after sigmoid: torch.Size([64, 256, 12])\n",
      "Output size after gated: torch.Size([64, 256, 12])\n",
      "Output size after res conv1d: torch.Size([64, 256, 12])\n",
      "Input cut size: torch.Size([64, 256, 12])\n",
      "Residual output size: torch.Size([64, 256, 12])\n",
      "Skip size before slicing: torch.Size([64, 256, 12])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 12])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 12])\n",
      "Output size after conv: torch.Size([64, 256, 5])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 5])\n",
      "Output size after tanh: torch.Size([64, 256, 5])\n",
      "Output size after sigmoid: torch.Size([64, 256, 5])\n",
      "Output size after gated: torch.Size([64, 256, 5])\n",
      "Output size after res conv1d: torch.Size([64, 256, 5])\n",
      "Input cut size: torch.Size([64, 256, 5])\n",
      "Residual output size: torch.Size([64, 256, 5])\n",
      "Skip size before slicing: torch.Size([64, 256, 5])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "Skip connection size: torch.Size([64, 256, 5])\n",
      "Skip connection size: torch.Size([64, 256, 4])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype = None)\n * (Tensor input, tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 37\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Y_batch\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:311\u001b[0m, in \u001b[0;36mWaveNet.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:304\u001b[0m, in \u001b[0;36mWaveNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m skip_connection \u001b[38;5;129;01min\u001b[39;00m skip_connections:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkip connection size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskip_connection\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_connections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput size after sum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenseLayer(output)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype = None)\n * (Tensor input, tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from layers import WaveNet\n",
    "\n",
    "X_dir = '../processed_chunks_16/X'\n",
    "Y_dir = '../processed_chunks_16/Y'\n",
    "ud, lossi = [], []\n",
    "\n",
    "model = WaveNet(4, 1, 256, 256)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_steps = 2000\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "audio_dataset = AudioDataset(X_dir, Y_dir)\n",
    "train_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    if step >= num_steps:\n",
    "        break\n",
    "    # Move batches to the desired device, e.g., GPU if available\n",
    "    # X_batch = torch.unsqueeze(X_batch, dim=1)\n",
    "    X_batch = X_batch.to('cpu')\n",
    "    # Y_batch = Y_batch.type(torch.LongTensor)\n",
    "    Y_batch = Y_batch.to('cpu')\n",
    "    X_batch = X_batch.permute(0, 2, 1)\n",
    "    print(X_batch.shape)\n",
    "    print(Y_batch.shape)\n",
    "    logits = model(X_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.cross_entropy(logits, Y_batch.long())\n",
    "    # for p in model.parameters():\n",
    "    #     p.grad = None\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"{step:7d}/{num_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in model.parameters()])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch.shape, Y_batch.shape, model.layers[1].weight.shape, logits.shape, logits\n",
    "X_batch.shape, Y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.dtype, Y_batch.dtype, model.layers[1].weight.dtype, logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_batch, Y_batch):\n",
    "    print(f\"X: {x.tolist()} -> Y: {y.item()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt      \n",
    "import numpy as np\n",
    "import torch    \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import librosa\n",
    "\n",
    "import librosa.display\n",
    "#from datasets import WaveNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1683).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_generate = 16000\n",
    "context = [0] * 8\n",
    "\n",
    "generated_samples = []\n",
    "\n",
    "for i in range(num_samples_to_generate):\n",
    "    # Forward pass: Get predictions for the current context\n",
    "    context_tensor = torch.nn.functional.one_hot(torch.tensor(context), 256).float().to(device)\n",
    "    logits = model(context_tensor.unsqueeze(0))  # Forward pass\n",
    "    probs = F.softmax(logits, dim=1)                          # Softmax for probabilities\n",
    "\n",
    "    # Sample from the probability distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()        # Sample next audio sample\n",
    "    generated_samples.append(ix)                               # Store generated sample\n",
    "\n",
    "    # Update the context window by shifting and adding the new sample\n",
    "    context = context[1:] + [ix] \n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i:7d}/{num_samples_to_generate:7d}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(generated_samples), min(generated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulaw_decode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device)\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device)\n",
    "    audio = (audio / mu) * 2 - 1\n",
    "    out = torch.sign(audio) * (torch.exp(torch.abs(audio) * torch.log1p(mu)) - 1) / mu\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_audio = mulaw_decode(torch.tensor(generated_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print max from decoded audio\n",
    "print(decoded_audio.max())\n",
    "print(decoded_audio.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io.wavfile import write\n",
    "\n",
    "# write('generated_audio.wav', 16000, decoded_audio.numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Play audio in Jupyter Notebook\n",
    "Audio(decoded_audio.cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Convert decoded_audio to a NumPy array if its a tensor\n",
    "if isinstance(decoded_audio, torch.Tensor):\n",
    "    decoded_audio_np = decoded_audio.cpu().numpy()  # Ensure it's on CPU and convert to numpy\n",
    "else:\n",
    "    decoded_audio_np = decoded_audio  # If it's already a numpy array, keep it as is\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(decoded_audio_np, sr=16000)  # Assuming a sample rate of 16kHz\n",
    "plt.title('Generated Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylim(-0.03, 0.03)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example audio from data_chunks .wav\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = 'data_chunks/chunk1_100.wav'\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(audio, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavenet-implementation-tchtkN5J-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
