{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import pretty_midi\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_law_encode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device)\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device)\n",
    "    # audio = torch.abs(torch.tensor(input))\n",
    "    # audio_abs = torch.min(torch.abs(audio), 1.0)\n",
    "    mag = torch.log1p(mu * torch.abs(audio)) / torch.log1p(mu)\n",
    "    signal = torch.sign(audio) * mag\n",
    "    out = ((signal + 1) / 2 * mu + 0.5)\n",
    "    out = torch.tensor(out, device=device)\n",
    "    return out\n",
    "\n",
    "def mulaw_decode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device).item()\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device).item(),\n",
    "    audio = (audio / mu) * 2 - 1\n",
    "    out = torch.sign(audio) * (torch.exp(torch.abs(audio) * torch.log1p(mu)) - 1) / mu\n",
    "    return out\n",
    "\n",
    "# def one_hot_encode(input, channels=256):\n",
    "#     size = input.size()[0]\n",
    "#     one_hot = torch.ShortTensor(size, channels)\n",
    "#     one_hot = one_hot.zero_()\n",
    "#     in1 = torch.tensor(input, dtype=torch.long, device='cpu')\n",
    "#     one_hot.scatter_(1, in1.unsqueeze(1), 1.0)\n",
    "#     one_hot = one_hot.to(device)\n",
    "#     return one_hot\n",
    "\n",
    "\n",
    "# def one_hot_decode(input):\n",
    "#     _, i = input.max(1)\n",
    "#     return torch.tensor(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for transforming .midi audio to .wav "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from midi2audio import FluidSynth\n",
    "\n",
    "# fs = FluidSynth()\n",
    "# # fs.midi_to_audio('notebooks/test.midi', 'notebooks/test3.wav')\n",
    "# midi_dir = 'maestro-v1.0.0/2004'\n",
    "# wav_dir = 'dataset'\n",
    "\n",
    "# os.makedirs(wav_dir, exist_ok=True)\n",
    "\n",
    "# # Loop through all MIDI files in the directory\n",
    "# for midi_file in os.listdir(midi_dir):\n",
    "#     if midi_file.endswith('.midi'):\n",
    "#         midi_path = os.path.join(midi_dir, midi_file)\n",
    "#         wav_path = os.path.join(wav_dir, midi_file.replace('.midi', '.wav'))\n",
    "        \n",
    "#         # Convert MIDI to WAV\n",
    "#         fs.midi_to_audio(midi_path, wav_path)\n",
    "#         print(f\"Converted {midi_path} to {wav_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for creating 1sec chunks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "\n",
    "# # Parameters\n",
    "# sample_rate = 16000  # Desired sample rate\n",
    "# chunk_duration = 1  # Duration of each chunk in seconds\n",
    "# chunk_length = sample_rate * chunk_duration  # Number of samples per chunk\n",
    "\n",
    "# # Directories\n",
    "# input_dir = 'dataset'  # Folder with your 200 audio files\n",
    "# output_dir = 'data_chunks'  # Folder to save the chunks\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# file_idx = 0\n",
    "# # Loop through all audio files\n",
    "# for audio_file in os.listdir(input_dir):\n",
    "#     if audio_file.endswith('.wav'):\n",
    "#         # Load the audio file\n",
    "#         audio_path = os.path.join(input_dir, audio_file)\n",
    "#         audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "\n",
    "#         # Split into chunks\n",
    "#         num_chunks = len(audio) // chunk_length\n",
    "#         for i in range(num_chunks):\n",
    "#             start_sample = i * chunk_length\n",
    "#             end_sample = start_sample + chunk_length\n",
    "#             chunk = audio[start_sample:end_sample]\n",
    "\n",
    "#             # Save the chunk\n",
    "#             chunk_filename = f\"chunk{file_idx}_{i}.wav\"\n",
    "#             chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "#             sf.write(chunk_path, chunk, sample_rate)\n",
    "#             print(f\"Saved {chunk_path}\")\n",
    "#         file_idx += 1\n",
    "#         # Do not save last partial chunk\n",
    "# print(\"All audio files have been split into chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for creating X, y pairs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "\n",
    "# block_size = 16\n",
    "# sample_rate = 16000 \n",
    "# chunk_dir = 'data_chunks' \n",
    "# output_dir_X = 'processed_chunks_16/X' \n",
    "# output_dir_Y = 'processed_chunks_16/Y'\n",
    "# os.makedirs(output_dir_X, exist_ok=True)\n",
    "# os.makedirs(output_dir_Y, exist_ok=True)\n",
    "\n",
    "# def build_and_save_pairs(audio, block_size,base_filename):\n",
    "#     context = torch.zeros(block_size, dtype=torch.int16, device=device)\n",
    "#     idx = 0\n",
    "#     for i in range(len(audio) - block_size):\n",
    "#         target = audio[i + block_size]\n",
    "#         X = torch.tensor(context, dtype=torch.int16, device=device)\n",
    "#         Y = torch.tensor(target, dtype=torch.int16, device=device)\n",
    "#         context = torch.cat((context[1:], torch.tensor([target], dtype=torch.int16, device=device)))\n",
    "\n",
    "#         X_filename = f\"{base_filename}_X_{idx}.pt\"\n",
    "#         Y_filename = f\"{base_filename}_Y_{idx}.pt\"\n",
    "#         X_path = os.path.join(output_dir_X, X_filename)\n",
    "#         Y_path = os.path.join(output_dir_Y, Y_filename)\n",
    "\n",
    "#         torch.save(X, X_path)\n",
    "#         torch.save(Y, Y_path)\n",
    "#         print(f\"Saved {X_path} and {Y_path}\")\n",
    "#         idx += 1\n",
    "\n",
    "\n",
    "# for chunk_file in os.listdir(chunk_dir):\n",
    "#     if chunk_file.endswith('.wav'):\n",
    "#         chunk_path = os.path.join(chunk_dir, chunk_file)\n",
    "        \n",
    "#         audio, sr = librosa.load(chunk_path, sr=sample_rate)\n",
    "\n",
    "#         audio = mu_law_encode(audio)\n",
    "\n",
    "#         base_filename = os.path.splitext(chunk_file)[0]\n",
    "        \n",
    "#         build_and_save_pairs(audio, block_size, base_filename)\n",
    "\n",
    "# print(\"Finished processing all chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check some X,y pairs\n",
    "# for i in range(100):\n",
    "#     X = torch.load(f'processed_chunks/X/chunk87_272_X_{i}.pt')\n",
    "#     y = torch.load(f'processed_chunks/Y/chunk87_272_Y_{i}.pt')\n",
    "#     print(f\"{X.tolist()} -> {y.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), device=device) / (fan_in**0.5) \n",
    "        self.bias = torch.zeros(fan_out, device=device) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "\n",
    "        self.running_mean = torch.zeros(dim, device=device)\n",
    "        self.running_var = torch.ones(dim, device=device)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "\n",
    "            xmean = x.mean(dim=dim, keepdim=True)\n",
    "            xvar = x.var(dim=dim, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class LeakyRelu:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.nn.functional.leaky_relu(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Relu:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.nn.functional.relu(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params, lr=0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "        self.beta1 = torch.tensor(beta1, device=device)\n",
    "        self.beta2 = torch.tensor(beta2, device=device)\n",
    "        self.eps = eps\n",
    "        self.m_d = {id(p) : torch.zeros_like(p) for p in params}\n",
    "        self.v_d = {id(p) : torch.zeros_like(p) for p in params}\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            grad = param.grad\n",
    "            m = self.m_d[id(param)]\n",
    "            v = self.v_d[id(param)]\n",
    "\n",
    "            next_m = (torch.multiply(self.beta1, m) + torch.multiply(1.0 - self.beta1, grad))\n",
    "            next_v = (torch.multiply(self.beta2, v) + torch.multiply(1.0 - self.beta2, torch.pow(grad, 2)))\n",
    "\n",
    "            m_hat = torch.divide(next_m, (1 - torch.pow(self.beta1, self.t)))\n",
    "            v_hat = torch.divide(next_v, (1 - torch.pow(self.beta2, self.t)))\n",
    "\n",
    "            param.data = param.data - torch.divide(torch.multiply(self.lr, m_hat), (torch.sqrt(v_hat) + self.eps))\n",
    "\n",
    "            self.m_d[id(param)] = next_m\n",
    "            self.v_d[id(param)] = next_v\n",
    "        self.t += 1\n",
    "    \n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "\n",
    "# n_embd = 24\n",
    "# n_hidden = 128\n",
    "# vocab_size = 256\n",
    "\n",
    "# model = Sequential([\n",
    "#     Flatten(2), Linear(vocab_size * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Flatten(2), Linear(n_hidden * 2, n_hidden, bias = False), BatchNorm1d(n_hidden), LeakyRelu(),\n",
    "#     Linear(n_hidden, vocab_size),\n",
    "# ])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.layers[-1].weight *= 0.1\n",
    "\n",
    "# # parameters = model.parameters()\n",
    "# # print(sum(p.nelement() for p in parameters))\n",
    "# # for p in parameters:\n",
    "# #     p.requires_grad = True\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import soundfile as sf \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X_dir, Y_dir):\n",
    "        self.X_dir = X_dir\n",
    "        self.Y_dir = Y_dir\n",
    "        self.X_files = sorted(os.listdir(X_dir))\n",
    "        self.Y_files = sorted(os.listdir(Y_dir))\n",
    "        \n",
    "        # Check if X and Y files match in length\n",
    "        assert len(self.X_files) == len(self.Y_files), \"Mismatch between X and Y files\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Total number of samples in the dataset\n",
    "        return len(self.X_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the X and Y tensors for the given index\n",
    "        X_path = os.path.join(self.X_dir, self.X_files[idx])\n",
    "        Y_path = os.path.join(self.Y_dir, self.Y_files[idx])\n",
    "        \n",
    "        # Load tensors from files\n",
    "        X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
    "        Y = torch.load(Y_path)  # Convert to long if needed\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.long)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "        X_one_hot = torch.nn.functional.one_hot(X, 256).float()\n",
    "        Y_one_hot = torch.nn.functional.one_hot(Y, 256).float()\n",
    "        \n",
    "        return X_one_hot,  Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/work/WaveNet_implementation/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74488/3689489596.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
      "/tmp/ipykernel_74488/3689489596.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(Y_path)  # Convert to long if needed\n",
      "/tmp/ipykernel_74488/3689489596.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.long)\n",
      "/tmp/ipykernel_74488/3689489596.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/   2000: 5.5452\n",
      "    100/   2000: 5.5452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Training Loop\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (X_batch, Y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_steps:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m Y_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_files[idx])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load tensors from files\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert to long if needed for compatibility with model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(Y_path)  \u001b[38;5;66;03m# Convert to long if needed\u001b[39;00m\n\u001b[1;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/_utils.py:202\u001b[0m, in \u001b[0;36m_rebuild_tensor_v2\u001b[0;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor_v2\u001b[39m(\n\u001b[1;32m    194\u001b[0m     storage,\n\u001b[1;32m    195\u001b[0m     storage_offset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ):\n\u001b[0;32m--> 202\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_rebuild_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m requires_grad\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavenet-implementation-74K-dzoQ-py3.10/lib/python3.10/site-packages/torch/_utils.py:175\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[0;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mstorage\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_untyped_storage\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from layers import WaveNet  # Make sure you have imported the correct WaveNet class\n",
    "\n",
    "# Ensure you are appending the correct path for the 'layers' module\n",
    "\n",
    "\n",
    "# Directories\n",
    "X_dir = '../processed_chunks_16/X'\n",
    "Y_dir = '../processed_chunks_16/Y'\n",
    "\n",
    "# Lists to keep track of loss and gradient statistics\n",
    "ud, lossi = [], []\n",
    "\n",
    "# Instantiate the model (you can adjust the parameters based on your task)\n",
    "model = WaveNet(256, 256, 2, 2, 3)  # Adjust this based on your configuration\n",
    "model.to('cuda')  # Make sure to put model on the correct device, use 'cuda' if you want to use GPU\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_steps = 2000\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "audio_dataset = AudioDataset(X_dir, Y_dir)  # Ensure AudioDataset is implemented correctly\n",
    "train_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "for step, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    if step >= num_steps:\n",
    "        break\n",
    "\n",
    "    # Move batches to the desired device, e.g., GPU if available\n",
    "    X_batch = X_batch.to('cuda')  # Move to GPU if using CUDA (e.g., .to('cuda'))\n",
    "    Y_batch = Y_batch.to('cuda')  # Same for Y_batch\n",
    "\n",
    "    # Permute to match input shape expected by the model (batch_size, channels, sequence_length)\n",
    "    X_batch = X_batch.permute(0, 2, 1)  # Adjust input shape to (batch_size, 256, seq_len)\n",
    "\n",
    "    # print(\"X_batch shape:\", X_batch.shape)\n",
    "    # print(\"Y_batch shape:\", Y_batch.shape)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    logits = model(X_batch)\n",
    "\n",
    "    loss = loss_fn(logits, Y_batch.view(-1, 1))  # Remove extra dimensions if necessary\n",
    "\n",
    "    # Reset gradients before backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss calculation: Use MSELoss for regression tasks\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        print(f\"{step:7d}/{num_steps:7d}: {loss.item():.4f}\")\n",
    "\n",
    "    # Track logs\n",
    "    lossi.append(loss.log10().item())  # Store the log10 of loss\n",
    "    # with torch.no_grad():\n",
    "    #     ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65224/2758831217.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load(X_path)  # Convert to long if needed for compatibility with model\n",
      "/tmp/ipykernel_65224/2758831217.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(Y_path)  # Convert to long if needed\n",
      "/tmp/ipykernel_65224/2758831217.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.long)\n",
      "/tmp/ipykernel_65224/2758831217.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 16])\n",
      "torch.Size([64])\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "Output size after conv: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual conv: torch.Size([64, 256, 16])\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "Output size after conv: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 16])\n",
      "Output size after tanh: torch.Size([64, 256, 16])\n",
      "Output size after sigmoid: torch.Size([64, 256, 16])\n",
      "Output size after gated: torch.Size([64, 256, 16])\n",
      "Output size after res conv1d: torch.Size([64, 256, 16])\n",
      "Input cut size: torch.Size([64, 256, 16])\n",
      "Residual output size: torch.Size([64, 256, 16])\n",
      "Skip size before slicing: torch.Size([64, 256, 16])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 16])\n",
      "Output size after conv: torch.Size([64, 256, 15])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 15])\n",
      "Output size after tanh: torch.Size([64, 256, 15])\n",
      "Output size after sigmoid: torch.Size([64, 256, 15])\n",
      "Output size after gated: torch.Size([64, 256, 15])\n",
      "Output size after res conv1d: torch.Size([64, 256, 15])\n",
      "Input cut size: torch.Size([64, 256, 15])\n",
      "Residual output size: torch.Size([64, 256, 15])\n",
      "Skip size before slicing: torch.Size([64, 256, 15])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 15])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 15])\n",
      "Output size after conv: torch.Size([64, 256, 12])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 12])\n",
      "Output size after tanh: torch.Size([64, 256, 12])\n",
      "Output size after sigmoid: torch.Size([64, 256, 12])\n",
      "Output size after gated: torch.Size([64, 256, 12])\n",
      "Output size after res conv1d: torch.Size([64, 256, 12])\n",
      "Input cut size: torch.Size([64, 256, 12])\n",
      "Residual output size: torch.Size([64, 256, 12])\n",
      "Skip size before slicing: torch.Size([64, 256, 12])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "----------RESIDUAL BLOCK--------\n",
      "Input size: torch.Size([64, 256, 12])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Input size: torch.Size([64, 256, 12])\n",
      "Output size after conv: torch.Size([64, 256, 5])\n",
      "----------CASUAL DILATED CONV--------\n",
      "Output size after casual dilated conv: torch.Size([64, 256, 5])\n",
      "Output size after tanh: torch.Size([64, 256, 5])\n",
      "Output size after sigmoid: torch.Size([64, 256, 5])\n",
      "Output size after gated: torch.Size([64, 256, 5])\n",
      "Output size after res conv1d: torch.Size([64, 256, 5])\n",
      "Input cut size: torch.Size([64, 256, 5])\n",
      "Residual output size: torch.Size([64, 256, 5])\n",
      "Skip size before slicing: torch.Size([64, 256, 5])\n",
      "Skip size = -1\n",
      "Skip size after slicing: torch.Size([64, 256, 1])\n",
      "----------RESIDUAL BLOCK--------\n",
      "Skip connection size: torch.Size([64, 256, 5])\n",
      "Skip connection size: torch.Size([64, 256, 4])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype = None)\n * (Tensor input, tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 37\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Y_batch\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:311\u001b[0m, in \u001b[0;36mWaveNet.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/WaveNet_implementation/layers.py:304\u001b[0m, in \u001b[0;36mWaveNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m skip_connection \u001b[38;5;129;01min\u001b[39;00m skip_connections:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkip connection size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskip_connection\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_connections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput size after sum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenseLayer(output)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype = None)\n * (Tensor input, tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from layers import WaveNet\n",
    "\n",
    "X_dir = '../processed_chunks_16/X'\n",
    "Y_dir = '../processed_chunks_16/Y'\n",
    "ud, lossi = [], []\n",
    "\n",
    "model = WaveNet(4, 1, 256, 256)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_steps = 2000\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "audio_dataset = AudioDataset(X_dir, Y_dir)\n",
    "train_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "    if step >= num_steps:\n",
    "        break\n",
    "    # Move batches to the desired device, e.g., GPU if available\n",
    "    # X_batch = torch.unsqueeze(X_batch, dim=1)\n",
    "    X_batch = X_batch.to('cpu')\n",
    "    # Y_batch = Y_batch.type(torch.LongTensor)\n",
    "    Y_batch = Y_batch.to('cpu')\n",
    "    X_batch = X_batch.permute(0, 2, 1)\n",
    "    print(X_batch.shape)\n",
    "    print(Y_batch.shape)\n",
    "    logits = model(X_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.cross_entropy(logits, Y_batch.long())\n",
    "    # for p in model.parameters():\n",
    "    #     p.grad = None\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"{step:7d}/{num_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in model.parameters()])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch.shape, Y_batch.shape, model.layers[1].weight.shape, logits.shape, logits\n",
    "X_batch.shape, Y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.dtype, Y_batch.dtype, model.layers[1].weight.dtype, logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_batch, Y_batch):\n",
    "    print(f\"X: {x.tolist()} -> Y: {y.item()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt      \n",
    "import numpy as np\n",
    "import torch    \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import librosa\n",
    "\n",
    "import librosa.display\n",
    "#from datasets import WaveNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1683).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_generate = 16000\n",
    "context = [0] * 8\n",
    "\n",
    "generated_samples = []\n",
    "\n",
    "for i in range(num_samples_to_generate):\n",
    "    # Forward pass: Get predictions for the current context\n",
    "    context_tensor = torch.nn.functional.one_hot(torch.tensor(context), 256).float().to(device)\n",
    "    logits = model(context_tensor.unsqueeze(0))  # Forward pass\n",
    "    probs = F.softmax(logits, dim=1)                          # Softmax for probabilities\n",
    "\n",
    "    # Sample from the probability distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()        # Sample next audio sample\n",
    "    generated_samples.append(ix)                               # Store generated sample\n",
    "\n",
    "    # Update the context window by shifting and adding the new sample\n",
    "    context = context[1:] + [ix] \n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i:7d}/{num_samples_to_generate:7d}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(generated_samples), min(generated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulaw_decode(input, channels=256):\n",
    "    n_q = channels - 1\n",
    "    mu = torch.tensor(n_q, dtype=torch.float, device=device)\n",
    "    audio = torch.tensor(input, dtype=torch.float, device=device)\n",
    "    audio = (audio / mu) * 2 - 1\n",
    "    out = torch.sign(audio) * (torch.exp(torch.abs(audio) * torch.log1p(mu)) - 1) / mu\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_audio = mulaw_decode(torch.tensor(generated_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print max from decoded audio\n",
    "print(decoded_audio.max())\n",
    "print(decoded_audio.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io.wavfile import write\n",
    "\n",
    "# write('generated_audio.wav', 16000, decoded_audio.numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Play audio in Jupyter Notebook\n",
    "Audio(decoded_audio.cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Convert decoded_audio to a NumPy array if it’s a tensor\n",
    "if isinstance(decoded_audio, torch.Tensor):\n",
    "    decoded_audio_np = decoded_audio.cpu().numpy()  # Ensure it's on CPU and convert to numpy\n",
    "else:\n",
    "    decoded_audio_np = decoded_audio  # If it's already a numpy array, keep it as is\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(decoded_audio_np, sr=16000)  # Assuming a sample rate of 16kHz\n",
    "plt.title('Generated Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylim(-0.03, 0.03)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example audio from data_chunks .wav\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = 'data_chunks/chunk1_100.wav'\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(audio, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavenet-implementation-74K-dzoQ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
